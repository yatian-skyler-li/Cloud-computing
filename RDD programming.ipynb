{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "\n",
      "THE SONNETS\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[71] at textFile at <console>:65\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load Shakespeare file into RDD\n",
    "val lines = sc.textFile(\"shakespeare.txt\")\n",
    "lines.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE SONNETS\n",
      "by William Shakespeare\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:27\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// remove empty lines\n",
    "val rdd1 = lines.filter(l => l.length>0)\n",
    "rdd1.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE SONNETS\n",
      "by William Shakespeare\n",
      "                     1\n",
      "  From fairest creatures we desire increase\n",
      "  That thereby beautys rose might never die\n",
      "  But as the riper should by time decease\n",
      "  His tender heir might bear his memory\n",
      "  But thou contracted to thine own bright eyes\n",
      "  Feedst thy lights flame with selfsubstantial fuel\n",
      "  Making a famine where abundance lies\n",
      "  Thy self thy foe to thy sweet self too cruel\n",
      "  Thou that art now the worlds fresh ornament\n",
      "  And only herald to the gaudy spring\n",
      "  Within thine own bud buriest thy content\n",
      "  And tender churl makst waste in niggarding\n",
      "    Pity the world or else this glutton be\n",
      "    To eat the worlds due by the grave and thee\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at map at <console>:27\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove the punctuations \n",
    "val rdd2 = rdd1.map(x=> x.replaceAll(\"\"\"[\\p{Punct}]\"\"\", \"\"))\n",
    "rdd2.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE\n",
      "SONNETS\n",
      "by\n",
      "William\n",
      "Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[74] at flatMap at <console>:66\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split the words\n",
    "val rdd3 = rdd2.flatMap(line => line.split(\" \"))\n",
    "rdd3.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE\n",
      "SONNETS\n",
      "by\n",
      "William\n",
      "Shakespeare\n",
      "1\n",
      "From\n",
      "fairest\n",
      "creatures\n",
      "we\n",
      "desire\n",
      "increase\n",
      "That\n",
      "thereby\n",
      "beautys\n",
      "rose\n",
      "might\n",
      "never\n",
      "die\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[75] at filter at <console>:66\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove empty words\n",
    "val rdd4 = rdd3.filter(_.nonEmpty)\n",
    "rdd4.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "the\n",
      "sonnets\n",
      "by\n",
      "william\n",
      "shakespeare\n",
      "1\n",
      "from\n",
      "fairest\n",
      "creatures\n",
      "we\n",
      "desire\n",
      "increase\n",
      "that\n",
      "thereby\n",
      "beautys\n",
      "rose\n",
      "might\n",
      "never\n",
      "die\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd5: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[76] at map at <console>:66\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// To lower case\n",
    "val rdd5 = rdd4.map(w => w.toLowerCase())\n",
    "rdd5.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abash\n",
      "abashed\n",
      "abashed\n",
      "abashes\n",
      "abashing\n",
      "abate\n",
      "abated\n",
      "abated\n",
      "abates\n",
      "abating\n",
      "abide\n",
      "abode\n",
      "abode\n",
      "abides\n",
      "abiding\n",
      "absorb\n",
      "absorbed\n",
      "absorbed\n",
      "absorbs\n",
      "absorbing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "verbs: Array[String] = Array(abash, abashed, abashed, abashes, abashing, abate, abated, abated, abates, abating, abide, abode, abode, abides, abiding, absorb, absorbed, absorbed, absorbs, absorbing, accept, accepted, accepted, accepts, accepting, accompany, accompanied, accompanied, accompanies, accompanying, ache, ached, ached, aches, aching, achieve, achieved, achieved, achieves, achieving, acquire, acquired, acquired, acquires, acquiring, act, acted, acted, acts, acting, add, added, added, adds, adding, address, addressed, addressed, addresses, addressing, adjust, adjusted, adjusted, adjusts, adjusting, admire, admired, admired, admires, admiring, admit, admitted, admitted, admits, admitting, advise, advised, advised, advises, advising, afford, afforded, afforded, affords, affording,...\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load verbs file\n",
    "// Create an RDD for the verbs\n",
    "val verbs = sc.textFile(\"all_verbs.txt\").collect()\n",
    "verbs.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desire\n",
      "increase\n",
      "rose\n",
      "die\n",
      "bear\n",
      "contracted\n",
      "own\n",
      "eyes\n",
      "lights\n",
      "making\n",
      "lies\n",
      "spring\n",
      "own\n",
      "waste\n",
      "be\n",
      "eat\n",
      "dig\n",
      "gazed\n",
      "be\n",
      "held\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd6: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[79] at filter at <console>:68\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filter out the non-verb words\n",
    "val rdd6 = rdd5.filter(x => verbs.contains(x))\n",
    "rdd6.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(desire,1)\n",
      "(increase,1)\n",
      "(rose,1)\n",
      "(die,1)\n",
      "(bear,1)\n",
      "(contracted,1)\n",
      "(own,1)\n",
      "(eyes,1)\n",
      "(lights,1)\n",
      "(making,1)\n",
      "(lies,1)\n",
      "(spring,1)\n",
      "(own,1)\n",
      "(waste,1)\n",
      "(be,1)\n",
      "(eat,1)\n",
      "(dig,1)\n",
      "(gazed,1)\n",
      "(be,1)\n",
      "(held,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd7: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at map at <console>:27\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generate key/value pairs\n",
    "val rdd7 = rdd6.map(x => (x,1))\n",
    "rdd7.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(float,2)\n",
      "(agree,20)\n",
      "(healing,2)\n",
      "(shot,45)\n",
      "(guide,24)\n",
      "(opening,11)\n",
      "(urging,9)\n",
      "(practises,1)\n",
      "(surge,9)\n",
      "(maintained,2)\n",
      "(counted,9)\n",
      "(carried,33)\n",
      "(order,92)\n",
      "(handled,4)\n",
      "(hidden,8)\n",
      "(shunning,2)\n",
      "(valuing,1)\n",
      "(stinks,1)\n",
      "(shaping,1)\n",
      "(hatches,7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd8: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:27\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Aggregate the word frequencies\n",
    "val rdd8 = rdd7.reduceByKey((a,b) => (a+b))\n",
    "rdd8.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\n",
       "dict: scala.io.BufferedSource = <iterator>\n",
       "verb_dict: Array[Array[String]] = Array(Array(abash, abash, abashed, abashed, abashes, abashing), Array(abate, abate, abated, abated, abates, abating), Array(abide, abide, abode, abode, abides, abiding), Array(absorb, absorb, absorbed, absorbed, absorbs, absorbing), Array(accept, accept, accepted, accepted, accepts, accepting), Array(accompany, accompany, accompanied, accompanied, accompanies, accompanying), Array(ache, ache, ached, ached, aches, aching), Array(achieve, achieve, achieved, achieved, achieves, achieving), Array(acquire, acquire, acquired, acquired, acquires, acquiring), Array(act, act, acted, acted, acts, acting), Array(add, add, added, added, adds, adding), Array(address, address, addressed, addressed, add...\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read file of verb_dict\n",
    "import scala.io.Source \n",
    "val dict = Source.fromFile(\"verb_dict.txt\")\n",
    "\n",
    "// use for loop to generate verb_dict : Array[Array[String]]\n",
    "var verb_dict : Array[Array[String]] = Array()\n",
    "var verb : Array[String] = Array()\n",
    "for (line <- dict.getLines()) {\n",
    "    var value = line.split(\",\")\n",
    "    verb = Array(value(0),value(1),value(2),value(3),value(4),value(5))\n",
    "    verb_dict = verb_dict ++ Array(verb)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(float,5)\n",
      "(engrave,1)\n",
      "(call,686)\n",
      "(offer,121)\n",
      "(agree,45)\n",
      "(guide,41)\n",
      "(sort,88)\n",
      "(surge,13)\n",
      "(improve,1)\n",
      "(include,4)\n",
      "(order,104)\n",
      "(type,3)\n",
      "(squeeze,3)\n",
      "(limp,7)\n",
      "(attend,223)\n",
      "(flee,90)\n",
      "(select,2)\n",
      "(prefer,21)\n",
      "(soothe,6)\n",
      "(contract,38)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "verbs2: org.apache.spark.rdd.RDD[(String, String)] = ShuffledRDD[98] at reduceByKey at <console>:77\n",
       "t2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[103] at reduceByKey at <console>:81\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// verb -> base verb (e.g had -> have)\n",
    "// Current verbs array generates have -> have twice, eliminates duplicate records\n",
    "val verbs2 = sc.parallelize(large)\n",
    "    .flatMap(v => v.map(v2 => (v2, v.head))) \n",
    "    .reduceByKey((k1, k2) => if (k1 == k2) k1 else k2) \n",
    "\n",
    "// Join two RDD by their keys \n",
    "//  Then sum all values that having same base verb \n",
    "val t2 = verbs2.join(rdd8).map(d => d._2).reduceByKey(_ + _) \n",
    "\n",
    "t2.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(be,26727)\n",
      "(have,7848)\n",
      "(do,6416)\n",
      "(come,3610)\n",
      "(make,2892)\n",
      "(love,2501)\n",
      "(go,2476)\n",
      "(let,2384)\n",
      "(say,2356)\n",
      "(know,2251)\n"
     ]
    }
   ],
   "source": [
    "// return the top 10 that are most frequently used verbs\n",
    "t2.sortBy(_._2,false).take(10).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
